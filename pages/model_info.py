"""
Model Information Page - Technical Details about the AI Model
Streamlit page displaying model specifications and performance metrics
"""

import streamlit as st
from model import model_info, get_statistics, get_prediction_history, model_exists
import matplotlib.pyplot as plt
import pandas as pd

# Page header
st.title("üî¨ Model Information")
st.markdown("Technical specifications and performance details of our accident severity classification model")

# Check if using fallback model
is_fallback = not model_exists()

if is_fallback:
    st.warning("""
    ‚ö†Ô∏è **Fallback Mode Active**
    
    The real TensorFlow model file was not found. The system is currently using a dummy model 
    that generates random predictions for demonstration purposes.
    
    **To use the real model:**
    - Place your trained model file at: `models/accident_severity_model.h5`
    - Ensure the file has proper permissions
    - Restart the application
    """)
    st.divider()

# Get model information
info = model_info()

# Model Overview
st.subheader("üìã Model Overview")

overview_col1, overview_col2 = st.columns(2)

with overview_col1:
    st.markdown("**Model Name**")
    st.code(info['model_name'])

    st.markdown("**Architecture**")
    st.code(info['architecture'])

    st.markdown("**Framework**")
    st.code(info['framework'])

with overview_col2:
    st.markdown("**Status**")
    st.success("‚úÖ Active & Operational")

    st.markdown("**Version**")
    st.code(info['version'])

    st.markdown("**Last Updated**")
    st.code(info['last_updated'])

# Architecture Details
st.divider()
st.subheader("üèóÔ∏è Architecture Details")

arch_col1, arch_col2, arch_col3 = st.columns(3)

with arch_col1:
    st.metric("Input Shape", str(info['input_shape']))
    st.metric("Number of Classes", info['num_classes'])

with arch_col2:
    st.metric("Training Epochs", info['training_epochs'])
    st.metric("Batch Size", info['batch_size'])

with arch_col3:
    st.metric("Optimizer", info['optimizer'])
    st.metric("Learning Rate", info['learning_rate'])

# Performance Metrics
st.divider()
st.subheader("üìä Performance Metrics")

metrics_col1, metrics_col2, metrics_col3 = st.columns(3)

with metrics_col1:
    st.metric("Accuracy", f"{info['accuracy']:.1f}%", delta="+2.1%")

with metrics_col2:
    st.metric("Precision", f"{info['precision']:.1f}%")

with metrics_col3:
    st.metric("Recall", f"{info['recall']:.1f}%")

# Performance Chart
st.divider()
st.subheader("üìà Model Performance")

# Create performance comparison chart
fig, ax = plt.subplots(figsize=(8, 4))

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
values = [info['accuracy'], info['precision'], info['recall'], info['f1_score']]
colors = ['#2196F3', '#4CAF50', '#FF9800', '#F44336']

bars = ax.bar(metrics, values, color=colors, alpha=0.7)
ax.set_ylabel('Percentage (%)')
ax.set_title('Model Performance Metrics')

# Add value labels on bars
for bar, value in zip(bars, values):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
            f'{value:.1f}%', ha='center', va='bottom')

ax.set_ylim(0, 100)

st.pyplot(fig)

# Training Data Information
st.divider()
st.subheader("üìö Training Data")

data_col1, data_col2 = st.columns(2)

with data_col1:
    st.markdown("**Dataset Size**")
    st.metric("Total Samples", info['training_samples'] + info['validation_samples'] + info['test_samples'])

    st.markdown("**Split Distribution**")
    training_size = info['training_samples']
    validation_size = info['validation_samples']
    test_size = info['test_samples']

with data_col2:
    # Create pie chart for data split
    fig, ax = plt.subplots(figsize=(5, 5))
    labels = ['Training', 'Validation', 'Test']
    sizes = [info['training_samples'], info['validation_samples'], info['test_samples']]
    colors_pie = ['#4CAF50', '#FF9800', '#F44336']

    ax.pie(sizes, labels=labels, colors=colors_pie, autopct='%1.1f%%', startangle=90)
    ax.axis('equal')
    ax.set_title('Dataset Split')

    st.pyplot(fig)

# Data distribution table
st.markdown("**Sample Distribution**")
split_data = pd.DataFrame({
    'Data Split': ['Training', 'Validation', 'Test', 'Total'],
    'Samples': [training_size, validation_size, test_size,
               training_size + validation_size + test_size],
    'Percentage': [f"{(training_size/(training_size+validation_size+test_size))*100:.1f}%",
                  f"{(validation_size/(training_size+validation_size+test_size))*100:.1f}%",
                  f"{(test_size/(training_size+validation_size+test_size))*100:.1f}%",
                  "100%"]
})
st.dataframe(split_data, use_container_width=True)

# Severity Classes
st.divider()
st.subheader("üè∑Ô∏è Classification Classes")

classes_data = [
    {"Class": "üü¢ Minor Damage", "ID": 0, "Description": "Scratches, dents, cosmetic damage"},
    {"Class": "üü° Moderate Damage", "ID": 1, "Description": "Structural damage, airbag deployment possible"},
    {"Class": "üî¥ Severe Crash", "ID": 2, "Description": "Major structural failure, potential total loss"}
]

st.dataframe(pd.DataFrame(classes_data), use_container_width=True)

# Model Capabilities
st.divider()
st.subheader("‚ö° Model Capabilities")

cap_col1, cap_col2 = st.columns(2)

with cap_col1:
    st.markdown("**Input Requirements**")
    st.info("""
    - Image formats: JPG, PNG, JPEG
    - Minimum resolution: 100x100 pixels
    - Maximum file size: 10MB
    - Color mode: RGB preferred
    - Aspect ratio: Any (auto-scaling)
    """)

with cap_col2:
    st.markdown("**Processing Features**")
    st.success("""
    - Real-time inference: ~1.8 seconds
    - Confidence scoring: 75-98% range
    - Batch processing: Single image
    - Memory usage: ~250MB
    - Platform: CPU/GPU compatible
    """)

# Technical Implementation Details
st.divider()
st.subheader("üîß Technical Implementation")

tech_col1, tech_col2 = st.columns(2)

with tech_col1:
    st.markdown("**Preprocessing Pipeline**")
    st.code("""
1. Image validation & format check
2. RGB conversion (if needed)
3. Resize to 224x224 pixels
4. Pixel normalization [0,1]
5. Batch dimension addition
    """)

with tech_col2:
    st.markdown("**Inference Pipeline**")
    st.code("""
1. Input validation
2. Model prediction
3. Confidence calculation
4. Class mapping & formatting
5. Result post-processing
    """)

# Model Limitations
st.divider()
st.subheader("‚ö†Ô∏è Model Limitations & Considerations")

lim_col1, lim_col2 = st.columns(2)

with lim_col1:
    st.warning("""
    **Known Limitations:**
    - Requires clear accident images
    - May struggle with extreme angles
    - Weather conditions affect accuracy
    - Occluded damage areas
    """)

with lim_col2:
    st.info("""
    **Best Practices:**
    - Use multiple angles when possible
    - Ensure good lighting conditions
    - Include context around damage
    - Avoid heavily filtered images
    """)

# Performance Notes
st.divider()
st.subheader("üöÄ Performance Notes")

perf_notes = """
- **Speed Optimization:** Model uses EfficientNet architecture for fast inference
- **Memory Efficiency:** Lightweight design suitable for web deployment
- **Accuracy Balance:** Optimized for practical real-world usage
- **Scalability:** Designed to handle multiple concurrent requests
- **Monitoring:** Built-in logging and performance tracking
"""

st.markdown(perf_notes)

# Future Improvements
st.divider()
st.subheader("üîÆ Future Improvements")

future_col1, future_col2 = st.columns(2)

with future_col1:
    st.markdown("**Planned Enhancements**")
    st.markdown("""
    - Multi-angle image support
    - Damage localization features
    - Integration with repair databases
    - Real-time video processing
    - Mobile app companion
    """)

with future_col2:
    st.markdown("**Research Areas**")
    st.markdown("""
    - Transfer learning improvements
    - Ensemble model approaches
    - Advanced data augmentation
    - Cross-vehicle compatibility
    - Weather condition adaptation
    """)

# Footer
st.divider()
st.caption("Model information is regularly updated to reflect the latest developments. " +
           "For technical inquiries, please contact the development team." +
           f"\n\nLast updated: {info['last_updated']}")

# Hidden debug info (expandable)
with st.expander("üîç Additional Technical Details"):
    st.json(info)

st.subheader("üì§ Export Model Stats & Predictions")
st.markdown("Download model metadata, statistics, and recent prediction history in CSV or JSON format.")

# Gather data
stats = get_statistics()
prediction_history = get_prediction_history(limit=100)

# Combine into dict
export_data = {
    "model_info": info,
    "statistics": stats,
    "prediction_history": [
        {
            "timestamp": p["timestamp"].isoformat(),
            "severity": p["severity"],
            "confidence": p["confidence"]
        } for p in prediction_history
    ]
}

import json, pandas as pd

# JSON download
json_str = json.dumps(export_data, indent=2)
st.download_button(
    label="Download JSON",
    data=json_str,
    file_name="model_export.json",
    mime="application/json"
)

# CSV download (flattened)
if prediction_history:
    df = pd.DataFrame([
        {
            "timestamp": p["timestamp"].strftime("%Y-%m-%d %H:%M:%S"),
            "severity": p["severity"],
            "confidence": p["confidence"]
        } for p in prediction_history
    ])
    csv_data = df.to_csv(index=False)
    st.download_button(
        label="Download Prediction History CSV",
        data=csv_data,
        file_name="prediction_history.csv",
        mime="text/csv"
    )
else:
    st.info("No prediction history available for CSV export.")

